import json
import re
import os
import requests
import time
import random
from urllib.parse import urlparse

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç¡®ä¿åœ¨ GitHub Actions ç¯å¢ƒä¸‹è·¯å¾„æ­£ç¡®
BASE_PATH = os.path.dirname(os.path.abspath(__file__))
JSON_DB = os.path.join(BASE_PATH, 'db.json')
OUTPUT_LIST = os.path.join(BASE_PATH, 'MyVideo.list')

def get_deep_domains(api_url):
    """
    é€šè¿‡ä¸‰æ¬¡éšæœºè¯·æ±‚æ•è·åŠ¨æ€ CDN åŸŸå
    """
    found_domains = set()
    found_keywords = set()
    
    for i in range(3):
        try:
            timestamp = int(time.time())
            nonce = random.randint(100, 999)
            target_url = f"{api_url}?ac=detail&pg=1&_t={timestamp}&_n={nonce}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            resp = requests.get(target_url, headers=headers, timeout=15)
            if resp.status_code == 200:
                data = resp.json()
                vod_list = data.get('list', [])
                for vod in vod_list:
                    play_url = vod.get('vod_play_url', '')
                    urls = re.findall(r'https?://[^\$,\s]+', play_url)
                    for u in urls:
                        domain = urlparse(u).netloc.split(':')[0]
                        if domain and len(domain) > 3:
                            found_domains.add(domain)
                            
                            parts = domain.split('.')
                            # æå–å‰ç¼€è¯æ ¹ (å¦‚ v12.qewbn.com -> v)
                            if len(parts) >= 3:
                                prefix = parts[0]
                                if re.match(r'^[a-z]{1,4}\d+$', prefix):
                                    keyword = re.sub(r'\d+', '', prefix)
                                    if len(keyword) >= 2:
                                        found_keywords.add(keyword)

                            # æå–ä¸»åŸŸæ ¸å¿ƒ (å¦‚ wwzycdn.10cong.com -> 10cong)
                            if len(parts) >= 2:
                                main_name = parts[-2]
                                if len(main_name) > 4: 
                                    found_keywords.add(main_name)
            time.sleep(0.3)
        except Exception as e:
            print(f"      âš ï¸ æ¢æµ‹å¤±è´¥: {e}")
            
    return found_domains, found_keywords

def generate():
    if not os.path.exists(JSON_DB):
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° db.json æ–‡ä»¶")
        return

    # --- 1. è¯»å–å†å²æ•°æ® (å¢é‡åˆå¹¶çš„æ ¸å¿ƒ) ---
    all_domains = set()
    all_keywords = {
        "m3u8", "yyv", "cdnlz", "yzzy", "wwzy", "10cong", "bfzy", "jszy", "360zy", "360zyx"
    } 
    
    if os.path.exists(OUTPUT_LIST):
        print(f"ğŸ“‚ å‘ç°ç°æœ‰è§„åˆ™ï¼Œæ­£åœ¨è¯»å–å†å²è®°å½•è¿›è¡Œå¢é‡åˆå¹¶...")
        with open(OUTPUT_LIST, 'r', encoding='utf-8') as f:
            for line in f:
                # å…¼å®¹ Clash Rule Provider æ ¼å¼æå–å†…å®¹
                kw_match = re.search(r'DOMAIN-KEYWORD,([^,\s]+)', line)
                sf_match = re.search(r'DOMAIN-SUFFIX,([^,\s]+)', line)
                if kw_match: all_keywords.add(kw_match.group(1).strip())
                if sf_match: all_domains.add(sf_match.group(1).strip())
        print(f"ğŸ“¥ å·²è½½å…¥å†å²: {len(all_keywords)} å…³é”®è¯, {len(all_domains)} åŸŸå")

    # --- 2. çˆ¬å–æ–°æ•°æ® ---
    with open(JSON_DB, 'r', encoding='utf-8') as f:
        db = json.load(f)

    sites = db.get('sites', [])
    print(f"ğŸš€ å¼€å§‹å¢é‡æ‰«æ {len(sites)} ä¸ªé‡‡é›†ç«™...")
    
    for site in sites:
        api = site.get('api', '')
        if not api or not api.startswith('http'): continue
            
        print(f"ğŸ” æ­£åœ¨æ¢æµ‹: {site.get('name', 'æœªçŸ¥ç«™')}")
        
        # å°† API è‡ªèº«çš„åŸŸåä¹ŸåŠ å…¥ç›´è¿
        api_host = urlparse(api).netloc.split(':')[0]
        if api_host: all_domains.add(api_host)
        
        domains, keywords = get_deep_domains(api)
        all_domains.update(domains)
        all_keywords.update(keywords)

    # --- 3. è¿‡æ»¤ä¸æŒä¹…åŒ– ---
    # è¿‡æ»¤æ‰é»‘åå•å’Œæ— æ•ˆè¯
    exclude = ["com", "net", "org", "www", "cdn", "index", "html", "payload", "github"]
    
    final_keywords = sorted([k for k in all_keywords if k and len(k) > 1 and k not in exclude])
    final_domains = sorted([d for d in all_domains if d and "." in d])

    with open(OUTPUT_LIST, 'w', encoding='utf-8') as f:
        f.write("payload:\n")
        print(f"âœï¸ å†™å…¥å…³é”®è¯ ({len(final_keywords)} æ¡)")
        for kw in final_keywords:
            f.write(f"  - DOMAIN-KEYWORD,{kw}\n")
        
        print(f"âœï¸ å†™å…¥åŸŸååç¼€ ({len(final_domains)} æ¡)")
        for d in final_domains:
            f.write(f"  - DOMAIN-SUFFIX,{d}\n")
            
    print(f"âœ… å¤„ç†å®Œæˆï¼æ€»è§„æ¨¡ï¼šåŸŸå {len(final_domains)}ï¼Œè¯æ ¹ {len(final_keywords)}")

if __name__ == "__main__":
    generate()
