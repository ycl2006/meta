import json
import re
import os
import requests
import time
import random
from urllib.parse import urlparse

# è·¯å¾„é…ç½®
BASE_PATH = os.path.dirname(os.path.abspath(__file__))
JSON_DB = os.path.join(BASE_PATH, 'db.json')
OUTPUT_LIST = os.path.join(BASE_PATH, 'MyVideo.list')

def get_deep_domains(api_url, site_name, existing_domains):
    found_domains = set()
    found_keywords = set()
    new_discoveries = []
    
    # å¢åŠ ä¸€ç‚¹éšæœºæ€§ç»•è¿‡åŸºç¡€é˜²ç«å¢™
    headers = {
        'User-Agent': 'okhttp/4.9.0',
        'Accept': 'application/json'
    }

    success = False
    for i in range(5): # å°è¯•5æ¬¡
        try:
            timestamp = int(time.time())
            nonce = random.randint(100, 999)
            target_url = f"{api_url}?ac=detail&pg=1&_t={timestamp}&_n={nonce}"
            
            resp = requests.get(target_url, headers=headers, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                vod_list = data.get('list', [])
                if not vod_list:
                    continue
                
                success = True
                for vod in vod_list:
                    play_url = vod.get('vod_play_url', '')
                    urls = re.findall(r'https?://[^\$,\s]+', play_url)
                    for u in urls:
                        domain = urlparse(u).netloc.split(':')[0]
                        if domain and len(domain) > 3:
                            found_domains.add(domain)
                            # å®æ—¶æ£€æŸ¥æ˜¯å¦æ˜¯æ–°åŸŸå
                            if domain not in existing_domains:
                                new_discoveries.append(domain)
                                existing_domains.add(domain) # é¿å…å•æ¬¡é‡å¤æ˜¾ç¤º
                            
                            # æå–å…³é”®å­—é€»è¾‘
                            parts = domain.split('.')
                            if len(parts) >= 3:
                                prefix = parts[0]
                                if re.match(r'^[a-z]{1,4}\d+$', prefix):
                                    kw = re.sub(r'\d+', '', prefix)
                                    if len(kw) >= 2: found_keywords.add(kw)
                            if len(parts) >= 2:
                                main_name = parts[-2]
                                if len(main_name) > 4: found_keywords.add(main_name)
                break 
            else:
                print(f"   âš ï¸  HTTP é”™è¯¯: {resp.status_code}")
        except Exception as e:
            if i == 2: print(f"   âŒ ç½‘ç»œå¼‚å¸¸: {str(e)}")
            continue
        time.sleep(1)

    return success, found_domains, found_keywords, new_discoveries

def generate():
    if not os.path.exists(JSON_DB):
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° db.json")
        return

    # --- 1. è¯»å–å†å²æ•°æ® ---
    all_domains, all_keywords = set(), {"m3u8", "yyv", "cdnlz", "yzzy", "wwzy", "10cong", "bfzy", "jszy", "360zy"}
    if os.path.exists(OUTPUT_LIST):
        with open(OUTPUT_LIST, 'r', encoding='utf-8') as f:
            content = f.read()
            all_keywords.update(re.findall(r'DOMAIN-KEYWORD,([^,\s]+)', content))
            all_domains.update(re.findall(r'DOMAIN-SUFFIX,([^,\s]+)', content))
    
    initial_dm_count = len(all_domains)
    print(f"ğŸ“¥ å†å²è½½å…¥: åŸŸååº“å·²æœ‰ {initial_dm_count} æ¡è®°å½•")

    # --- 2. çˆ¬å–æ–°æ•°æ® ---
    with open(JSON_DB, 'r', encoding='utf-8') as f:
        db = json.load(f)
    
    sites = db.get('sites', [])
    total = len(sites)
    print(f"ğŸš€ å¼€å§‹æ‰«æ {total} ä¸ªé‡‡é›†ç«™...")

    for i, site in enumerate(sites, 1):
        name = site.get('name', 'æœªçŸ¥ç«™')
        api = site.get('api', '')
        
        # å®æ—¶æ˜¾ç¤ºæ¢æµ‹çŠ¶æ€
        print(f"[{i}/{total}] æ­£åœ¨æ¢æµ‹: {name} ", end="", flush=True)
        
        if api and api.startswith('http'):
            # è®°å½•æ¥å£ä¸»åŸŸå
            api_host = urlparse(api).netloc.split(':')[0]
            if api_host: all_domains.add(api_host)
            
            # æ·±å…¥æ¢æµ‹
            is_ok, domains, keywords, news = get_deep_domains(api, name, all_domains)
            
            if is_ok:
                print(f"âœ… [æˆåŠŸ]")
                if news:
                    for d in news:
                        print(f"   âœ¨ å‘ç°æ–°åŸŸå: {d}")
                all_keywords.update(keywords)
            else:
                print(f"âŒ [å¤±è´¥æˆ–æ— æ•°æ®]")
        else:
            print(f"â© [è·³è¿‡: æ— æ•ˆAPI]")

    # --- 3. ç»ˆæå»é‡é€»è¾‘ ---
    exclude = ["com", "net", "org", "www", "cdn", "index", "html", "payload", "github", "vip"]
    processed_keywords = set()
    for k in all_keywords:
        if not k or len(k) <= 1 or k in exclude: continue
        base = re.sub(r'\d+$', '', k)
        processed_keywords.add(base if len(base) > 2 else k)

    final_keywords = sorted(list(processed_keywords))
    final_domains = []
    sorted_raw_domains = sorted(list(all_domains), key=len)
    for d in sorted_raw_domains:
        if not d or "." not in d: continue
        if not any(kw in d for kw in final_keywords):
            if not any(d.endswith("." + x) for x in final_domains):
                final_domains.append(d)

    # --- 4. å†™å…¥æ–‡ä»¶ ---
    with open(OUTPUT_LIST, 'w', encoding='utf-8') as f:
        f.write("payload:\n")
        for kw in final_keywords: f.write(f"  - DOMAIN-KEYWORD,{kw}\n")
        for d in sorted(final_domains): f.write(f"  - DOMAIN-SUFFIX,{d}\n")

    # --- 5. ç»Ÿè®¡æŠ¥å‘Š ---
    added_dm = len(final_domains) - initial_dm_count
    print("\n" + "="*40)
    print(f"ğŸ‰ æ‰«æä»»åŠ¡å®Œæˆ!")
    print(f"âœ¨ æœ¬æ¬¡æ–°æ”¶å‰²åŸŸå: {max(0, added_dm)} æ¡")
    print(f"ğŸ“¦ è§„åˆ™æ–‡ä»¶å·²æ›´æ–°: {OUTPUT_LIST}")
    print("="*40)

if __name__ == "__main__":
    generate()
